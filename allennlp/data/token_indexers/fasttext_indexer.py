from typing import Dict, List
import itertools

from overrides import overrides

from allennlp.data.vocabulary import Vocabulary
from allennlp.data.tokenizers.token import Token
from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList
import fasttext
import numpy as np
import torch
from allennlp.common.util import pad_sequence_to_length


@TokenIndexer.register("fasttext")
class FasttextTokenIndexer(TokenIndexer):
    """
    This :class:`TokenIndexer` represents tokens by dynamically computed fasttext embeddings

    # Parameters

    namespace : `str`, optional (default=`tokens`)
        We will use this namespace in the :class:`Vocabulary` to map strings to indices.
    lowercase_tokens : `bool`, optional (default=`False`)
        If `True`, we will call `token.lower()` before getting an index for the token from the
        vocabulary.
    start_tokens : `List[str]`, optional (default=`None`)
        These are prepended to the tokens provided to `tokens_to_indices`.
    end_tokens : `List[str]`, optional (default=`None`)
        These are appended to the tokens provided to `tokens_to_indices`.
    feature_name : `str`, optional (default=`text`)
        We will use the :class:`Token` attribute with this name as input
    token_min_padding_length : `int`, optional (default=`0`)
        See :class:`TokenIndexer`.
    """

    def __init__(
            self,
            model_path: str,
            embedding_dim,
            namespace: str = "tokens",
            start_tokens: List[str] = None,
            end_tokens: List[str] = None,
            feature_name: str = "text",
            token_min_padding_length: int = 0,
    ) -> None:
        super().__init__(token_min_padding_length)
        self.model_path = model_path
        self.namespace = namespace
        self._start_tokens = [Token(st) for st in (start_tokens or [])]
        self._end_tokens = [Token(et) for et in (end_tokens or [])]
        self.feature_name = feature_name
        self.model = fasttext.load_model(self.model_path)
        self.embedding_dim = embedding_dim
        self.padding_value = np.zeros(embedding_dim)

    @overrides
    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
        pass

    def embed(self, text):
        return self.model.get_word_vector(text)

    @overrides
    def tokens_to_indices(
            self, tokens: List[Token], vocabulary: Vocabulary
    ) -> Dict[str, List[float]]:
        embeddings_list: List[float] = []
        # we are hacking a little bit around here, the idx is not generated by the vocabulary but by the
        # coordinate lookup
        for token in itertools.chain(self._start_tokens, tokens, self._end_tokens):
            text = getattr(token, self.feature_name)

            embeddings = self.embed(text)
            embeddings_list.append(embeddings)

        return {"token_fasttext": embeddings_list}

    def as_padded_tensor_dict(
            self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]
    ) -> Dict[str, torch.Tensor]:
        """
        This method pads a list of tokens given the input padding lengths (which could actually
        truncate things, depending on settings) and returns that padded list of input tokens as a
        `Dict[str, torch.Tensor]`.  This is a dictionary because there should be one key per
        argument that the `TokenEmbedder` corresponding to this class expects in its `forward()`
        method (where the argument name in the `TokenEmbedder` needs to make the key in this
        dictionary).

        The base class implements the case when all you want to do is create a padded `LongTensor`
        for every list in the `tokens` dictionary.  If your `TokenIndexer` needs more complex
        logic than that, you need to override this method.
        """
        tensor_dict = {}
        for key, val in tokens.items():
            padded_sequence = pad_sequence_to_length(val, padding_lengths[key],
                                                     default_value=lambda: self.padding_value)
            tensor_dict[key] = torch.FloatTensor(padded_sequence)
        return tensor_dict

    @overrides
    def get_empty_token_list(self) -> IndexedTokenList:
        return {"token_fasttext": []}
